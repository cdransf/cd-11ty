---
date: '2024-01-31'
title: "I don't want anything your AI generates"
description: 'AI output is fundamentally derivative and exploitative (of content, labor and the environment).'
tags: ['tech']
---
I really don't. AI output is fundamentally derivative and exploitative (of content, labor and the environment).<!-- excerpt -->

I can't trust the answers it provides or the text it generates. It's not a replacement for search, it simply makes search worse.

The images it generates are, at best, a polished regression to the mean. If you want custom art, pay an artist.

I want to talk to a person, not a chatbot. The chatbot wastes time while you wait for a person that can actually help.

I don't want music recommendations from something that can't appreciate or understand music. Human recommendations will always be better.

I don't want AI mediating social interactions that it cannot and does not understand (though it may appear to). If I'm weary of too much volume on any social platform or in any news feed, I'll cut back on what I'm following.

If you're having AI attend a meeting for you, it probably wasn't that important. If you're having AI write your email, it probably wasn't that important. If it's screening job candidates for you, you're missing quality candidates.

I don't like the idea of it being trained on anything I've written or created[^1].

I'll concede that Copilot is better than traditional autocomplete, but that feels like a pretty low bar[^2].

These tools will improve[^3]. The cost to the environment will increase, it will help companies deskill jobs and muddy public discourse. The "benefits" it provides to end users are, at best, dubious â€” though everyone responsible for creating it will most certainly enrich themselves.

This all portends a future I can't imagine wanting or even being interested in.

[^1]: Although consuming that may well be to its detriment. Nonetheless, my `robots.txt` file reflects this stance.
[^2]: My first experience with Copilot chat was it getting an answer wrong, apologizing when that was pointed out and then repeating the same or similar, but still incorrect, answer.
[^3]: I don't think this is a good thing.